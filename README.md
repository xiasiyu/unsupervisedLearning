- conda create -n unsupervised python=3.6 anaconda
- conda activate unsupervised
- Scikit-Learn进行无监督学习
    - **降维**
        - 维数的诅咒指由于特征空间的绝对大小，算法无法有效和高效地训练数据
        - 降维算法将数据投影到低维空间，再删除冗余信息的同时尽可能保留突出信息。
    - 线性投影。linear projection。包括诸如PCA，奇异值分解和随机投影
        - 主成分分析PCA
            - PCA之前必须执行功能缩放，因为PCA对原始特征的相对范围非常敏感。缩放到标准尺度。
            - 普通PCA
                - 通过处理特征之间的相关性来，如果特征子集之间的相关性很高，普通PCA将尝试组合高度相关的特征，并用较少的不相关线性特征表示该数据。
                - 在原始高纬度数据中找到最大方差的方向，并将它们投影到较小的维度空间之中。这些新派生的组件成为主成分。
            - 增量PCA
                - 数据量过大，无法在内存中容纳。可以通过小批量递增的方式执行PCA。
            - 稀疏PCA
                - 稀疏表示保留了某种稀疏度，由alpha的超参数控制稀疏度。只在一部分输入变量中搜索线性组合，在一定程度上减少原始特征空间的同时结果不像普通PCA那么紧凑。
            - 核PCA
                - 在原始数据上运行一个相似函数，以进行非线性降维。通过学习这个kernel method，核PCA将部分数据点映射到隐使特征空间，并以比原始特征集中的维数更小的维数创建隐式特征空间。**当原始特征空间不能线性分离的时候，该方法尤其有效。**
                - 设置所需的成分数量，核类型和核系数（gamma值）。最流行的核是径向基函数核，radial basic functional kernel，简称RBF核。
            - 奇异值分解SVD
                - 学习数据底层结构的另一种方法是将特征的原始矩阵的秩（极大线性无关组的个数）降到较小的秩。这样可以使用较小秩距震中的某些向量的线性组合重新创建原始矩阵。欧英
            - 随机投影
                - 依赖于Johnson-Lindenstrauss引理。高维空间中的点可以嵌入到一个低维空间中。这样点之间的距离几乎可以保持相对不变。高维转换为低维之后，原始特征集的结构也保留了。
                    - 标准本：高斯随机投影
                        - 可以指定缩减的特征空间中希望拥有的成分数量，或者设置超参数eps，eps用于控制嵌入的质量，较小的数值生成更多的维度
                    - 稀疏版本：系数随机投影
    - 流形学习，也被称为非线性降维，涉及的技术包括isomap（等距映射，测地距离），多维标度法，局部线性嵌入（LLE），t-分布随机领域嵌入（T-SNE），字典学习，随机树嵌入和独立成分分析
        - 等距映射 isomap。通过计算所有的点的成对距离来学习原始特征集的新的低维嵌入。测地距离
        - MDS，多维标度法。学习原始数据中点的相似性，在低维空间建模
        - LLE，局部线性嵌入。将数据从原始特征空间投影到降维的空间时，保持了局部领域内的距离。LLE将高位数据分割成更小的组件，并为每个组件建模为线性嵌入，来发现非线性结构
        - t-SNE：将每个高维点建模成一个二维或者三维空间来实现，在二维或者三维空间中，相似点彼此建模，不同的点在远处建模。通过构造两个概率分布来实现这一点。相似点具有高概率。最小化了两种概率分部之间的KL散度。
            - 实际应用中，在使用t-SNE之前，最好使用另一种降维技术比如PCA来减少维的数量。降低噪声。
            - 具有非凸函数，算法的不同初始化将产生不同的结果，不稳定
    - 其他不依赖于任何几何或者度量距离的方法
        - 字典学习：学习原始矩阵的稀疏表示，生成的矩阵称为字典。向量称为原子。
        - 独立成分分析：ICA